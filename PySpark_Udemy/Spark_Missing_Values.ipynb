{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899c1312-e1fd-4107-acce-f3605b792d97",
   "metadata": {},
   "source": [
    "### Handling Missing Values in a Dataset\n",
    "\n",
    "You can use the .na functions for missing data. The drop command has the following parameters:\n",
    "\n",
    "df.na.drop(how='any', thresh=None, subset=None)\n",
    "\n",
    "* param how: 'any' or 'all'.\n",
    "\n",
    "    If 'any', drop a row if it contains any nulls.\n",
    "    If 'all', drop a row only if all its values are null.\n",
    "\n",
    "* param thresh: int, default None\n",
    "\n",
    "    If specified, drop rows that have less than `thresh` non-null values.\n",
    "    This overwrites the `how` parameter.\n",
    "\n",
    "* param subset: \n",
    "    optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3882e0b-77c2-488a-bd56-ba8b0071c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9564d3-eb31-413d-a530-bc6303222df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/28 16:58:04 WARN Utils: Your hostname, myspark resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/04/28 16:58:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/28 16:58:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"missingdata\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64801161-f6c7-48e4-8490-42cb4f05a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/home/vboxuser/jupyter_notebooks/Python-and-Spark-for-Big-Data-master/Spark_DataFrames/ContainsNull.csv\",\n",
    "                     header = True,\n",
    "                     inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed27a293-f287-46a5-be6f-e57409b25315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f3dc3b5-cc3d-405d-bdc6-0bd481c1bd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d011ceb-4d08-4564-be73-b56af1fb2cfd",
   "metadata": {},
   "source": [
    "Thresh attribute specifies the number of not null columns that should be populated in a row.\n",
    "If there are less than thresh number of not null columns then those rows are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c0123f-cdc6-4bf4-b42b-c0f05a6f495d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77eb1740-634a-4ec5-b695-afcd0abb9550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# thresh = 3 means at least 3 columns should be populated for a row in the data frame.\n",
    "\n",
    "df.na.drop(thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67fe94d9-356b-4e2f-9c05-97478d14af49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how = 'any' means drop a row if any of the column values is null\n",
    "\n",
    "df.na.drop(how = 'any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f89a1a0-3515-4177-b97b-000de4cd088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how = 'all' means drop a row if all it's columns are null\n",
    "\n",
    "df.na.drop(how = 'all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c474eed1-6bac-4646-ba19-c40a8737adb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset = \"Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed3e001-248b-4693-b8ac-0c5388eaa7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset = [\"Sales\",\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7600ec27-814c-4a21-9696-acd2b438fff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| NULL|\n",
      "|emp2|No Name| NULL|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"No Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10b2e11-6f3a-4b52-aa67-829a3e1174b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| NULL|\n",
      "|emp2|No Name| NULL|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"No Name\",\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16b211f-2973-488f-9826-25417fa6016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| NULL|  0.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0,\"Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13ffe61f-77ab-4a92-95c3-a9f88842e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee2c1f51-7c9d-41ad-b50a-cfccec78c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanval = df.select(mean(df[\"Sales\"])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31bfcc9a-07df-471c-aedd-0329570912b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = meanval[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4305fef-0740-401b-972e-46f645ce7e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400.5\n"
     ]
    }
   ],
   "source": [
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7b7123d-24ec-48a0-98f8-ceb948941770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| NULL|400.5|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(mean,\"Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf365b2-cc10-4407-b06e-feb6a3fd84b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
