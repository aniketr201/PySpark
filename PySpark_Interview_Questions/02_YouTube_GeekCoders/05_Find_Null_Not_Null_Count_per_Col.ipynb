{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "729ad3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------------------+-------------+\n",
      "| id|user_id|first_login_timestamp|     location|\n",
      "+---+-------+---------------------+-------------+\n",
      "|  1| user_1|  2023-01-01 08:00:00|     New York|\n",
      "|  2|   null|  2023-01-02 09:00:00|  Los Angeles|\n",
      "|  3| user_3|                 null|      Chicago|\n",
      "|  4| user_4|  2023-01-04 10:00:00|         null|\n",
      "|  5|   null|                 null|      Houston|\n",
      "|  6| user_6|  2023-01-06 11:00:00|      Phoenix|\n",
      "|  7|   null|  2023-01-07 12:00:00|         null|\n",
      "|  8| user_8|                 null| Philadelphia|\n",
      "|  9| user_9|  2023-01-09 13:00:00|         null|\n",
      "| 10|   null|                 null|San Francisco|\n",
      "+---+-------+---------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateDFWithPartialNulls\").getOrCreate()\n",
    "\n",
    "# Define schema with columns: id, user_id, first_login_timestamp, location\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"first_login_timestamp\", TimestampType(), True),\n",
    "    StructField(\"location\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create data with some null values\n",
    "data = [\n",
    "    (1, \"user_1\", datetime(2023, 1, 1, 8, 0, 0), \"New York\"),\n",
    "    (2, None, datetime(2023, 1, 2, 9, 0, 0), \"Los Angeles\"),\n",
    "    (3, \"user_3\", None, \"Chicago\"),\n",
    "    (4, \"user_4\", datetime(2023, 1, 4, 10, 0, 0), None),\n",
    "    (5, None, None, \"Houston\"),\n",
    "    (6, \"user_6\", datetime(2023, 1, 6, 11, 0, 0), \"Phoenix\"),\n",
    "    (7, None, datetime(2023, 1, 7, 12, 0, 0), None),\n",
    "    (8, \"user_8\", None, \"Philadelphia\"),\n",
    "    (9, \"user_9\", datetime(2023, 1, 9, 13, 0, 0), None),\n",
    "    (10, None, None, \"San Francisco\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e98c62",
   "metadata": {},
   "source": [
    "### Static Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e78dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, isnull, col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a365d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------+--------------+\n",
      "|total_count|user_id_count|flt_count|location_count|\n",
      "+-----------+-------------+---------+--------------+\n",
      "|         10|            6|        6|             7|\n",
      "+-----------+-------------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count(\"*\").alias(\"total_count\")\n",
    "          , count(\"user_id\").alias(\"user_id_count\")\n",
    "          , count(\"first_login_timestamp\").alias(\"flt_count\")\n",
    "          , count(\"location\").alias(\"location_count\")\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2580371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------------+--------------------+\n",
      "|total_count|user_nulls_count|flt_nulls_count|location_nulls_count|\n",
      "+-----------+----------------+---------------+--------------------+\n",
      "|         10|               4|              4|                   3|\n",
      "+-----------+----------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count(\"*\").alias(\"total_count\")\n",
    "            , count(when(isnull(col(\"user_id\")),1\n",
    "                        )\n",
    "                   ).alias(\"user_nulls_count\")\n",
    "            , count(when(isnull(col(\"first_login_timestamp\")),1\n",
    "                        )\n",
    "                   ).alias(\"flt_nulls_count\")\n",
    "            , count(when(isnull(col(\"location\")),1\n",
    "                        )\n",
    "                   ).alias(\"location_nulls_count\")\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3b84f",
   "metadata": {},
   "source": [
    "#### To do this dynamically for all the columns, Python List Comprehension and Unpacking Operator is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cefed4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'user_1', datetime.datetime(2023, 1, 1, 8, 0), 'New York'), (2, None, datetime.datetime(2023, 1, 2, 9, 0), 'Los Angeles'), (3, 'user_3', None, 'Chicago'), (4, 'user_4', datetime.datetime(2023, 1, 4, 10, 0), None), (5, None, None, 'Houston'), (6, 'user_6', datetime.datetime(2023, 1, 6, 11, 0), 'Phoenix'), (7, None, datetime.datetime(2023, 1, 7, 12, 0), None), (8, 'user_8', None, 'Philadelphia'), (9, 'user_9', datetime.datetime(2023, 1, 9, 13, 0), None), (10, None, None, 'San Francisco')]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "279c59b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_1\n",
      "None\n",
      "user_3\n",
      "user_4\n",
      "None\n",
      "user_6\n",
      "None\n",
      "user_8\n",
      "user_9\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Regular Print for user_id\n",
    "\n",
    "for x in data:\n",
    "    print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9312a33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_1', None, 'user_3', 'user_4', None, 'user_6', None, 'user_8', 'user_9', None]\n"
     ]
    }
   ],
   "source": [
    "# Print with List Comprehension\n",
    "# Creates as list on the go, applies the logic on left of \"for\" on the elements in \"data\"\n",
    "\n",
    "print([x[1] for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4347f8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_1 None user_3 user_4 None user_6 None user_8 user_9 None\n"
     ]
    }
   ],
   "source": [
    "# Unpacking operator (*) just unpacks the list into individual int or str\n",
    "\n",
    "print(*[x[1] for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed09fc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'user_id', 'first_login_timestamp', 'location']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ca5a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_list = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e31749de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'user_id', 'first_login_timestamp', 'location']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84212bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "779b5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_counts = df.agg( *[ count(col(col_name)) for col_name in df.columns ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e27ac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------------------------+---------------+\n",
      "|count(id)|count(user_id)|count(first_login_timestamp)|count(location)|\n",
      "+---------+--------------+----------------------------+---------------+\n",
      "|       10|             6|                           6|              7|\n",
      "+---------+--------------+----------------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7107dc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id_nulls_count</th><th>user_id_nulls_count</th><th>first_login_timestamp_nulls_count</th><th>location_nulls_count</th></tr>\n",
       "<tr><td>0</td><td>4</td><td>4</td><td>3</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------+-------------------+---------------------------------+--------------------+\n",
       "|id_nulls_count|user_id_nulls_count|first_login_timestamp_nulls_count|location_nulls_count|\n",
       "+--------------+-------------------+---------------------------------+--------------------+\n",
       "|             0|                  4|                                4|                   3|\n",
       "+--------------+-------------------+---------------------------------+--------------------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate all null counts into a single row\n",
    "df.agg(*[count(when(col(x).isNull(), 1)).alias(f\"{x}_nulls_count\") for x in df.columns])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
