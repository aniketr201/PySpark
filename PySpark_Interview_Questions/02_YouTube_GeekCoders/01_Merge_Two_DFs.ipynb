{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e122ed15",
   "metadata": {},
   "source": [
    "### 1. Merge Two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "165759a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa97d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Interview_Questions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5330bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_columns = [\"id\", \"name\", \"age\", \"salary\", \"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3710cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data for DataFrame with 5 columns\n",
    "data1 = [\n",
    "    (1, \"Alice\", 28, 70000.00, \"New York\"),\n",
    "    (2, \"Bob\", 34, 85000.50, \"Los Angeles\"),\n",
    "    (3, \"Cathy\", 29, 95000.00, \"Chicago\"),\n",
    "    (4, \"David\", 41, 123000.00, \"San Francisco\"),\n",
    "    (5, \"Eva\", 36, 110000.00, \"Seattle\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df7439c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(data1, schema = df1_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5b7e89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+--------+-------------+\n",
      "| id| name|age|  salary|         city|\n",
      "+---+-----+---+--------+-------------+\n",
      "|  1|Alice| 28| 70000.0|     New York|\n",
      "|  2|  Bob| 34| 85000.5|  Los Angeles|\n",
      "|  3|Cathy| 29| 95000.0|      Chicago|\n",
      "|  4|David| 41|123000.0|San Francisco|\n",
      "|  5|  Eva| 36|110000.0|      Seattle|\n",
      "+---+-----+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff971fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15923fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_columns = [\"id\", \"name\", \"age\", \"salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a85de1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [\n",
    "    (6, \"Aniket\", 28, 70000.00),\n",
    "    (7, \"Swati\", 34, 85000.50),\n",
    "    (3, \"Anmol\", 29, 95000.00)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf01e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(data2, schema = df2_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79c25dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+-------+\n",
      "| id|  name|age| salary|\n",
      "+---+------+---+-------+\n",
      "|  6|Aniket| 28|70000.0|\n",
      "|  7| Swati| 34|85000.5|\n",
      "|  3| Anmol| 29|95000.0|\n",
      "+---+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c1674f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ae9d479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "400252c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Union can only be performed on tables with the same number of columns, but the first table has 5 columns and the second table has 4 columns;\n'Union false, false\n:- LogicalRDD [id#104L, name#105, age#106L, salary#107, city#108], false\n+- LogicalRDD [id#135L, name#136, age#137L, salary#138], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-38cde0c2f6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSQL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mresolves\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mby\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \"\"\"\n\u001b[0;32m-> 1828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Union can only be performed on tables with the same number of columns, but the first table has 5 columns and the second table has 4 columns;\n'Union false, false\n:- LogicalRDD [id#104L, name#105, age#106L, salary#107, city#108], false\n+- LogicalRDD [id#135L, name#136, age#137L, salary#138], false\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c72a0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7e9571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"city\",lit(\"Null\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43730b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+-------+----+\n",
      "| id|  name|age| salary|city|\n",
      "+---+------+---+-------+----+\n",
      "|  6|Aniket| 28|70000.0|Null|\n",
      "|  7| Swati| 34|85000.5|Null|\n",
      "|  3| Anmol| 29|95000.0|Null|\n",
      "+---+------+---+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f79c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d049b058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+--------+-------------+\n",
      "| id|  name|age|  salary|         city|\n",
      "+---+------+---+--------+-------------+\n",
      "|  1| Alice| 28| 70000.0|     New York|\n",
      "|  2|   Bob| 34| 85000.5|  Los Angeles|\n",
      "|  3| Cathy| 29| 95000.0|      Chicago|\n",
      "|  4| David| 41|123000.0|San Francisco|\n",
      "|  5|   Eva| 36|110000.0|      Seattle|\n",
      "|  6|Aniket| 28| 70000.0|         Null|\n",
      "|  7| Swati| 34| 85000.5|         Null|\n",
      "|  3| Anmol| 29| 95000.0|         Null|\n",
      "+---+------+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b24b88",
   "metadata": {},
   "source": [
    "### What a column in between is missing? Like Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65adff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_columns = [\"id\", \"name\", \"salary\", \"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44260da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = [\n",
    "    (6, \"Aniket\", 70000.00,\"Pune\"),\n",
    "    (7, \"Swati\", 85000.50, \"Hyderabad\"),\n",
    "    (3, \"Anmol\", 95000.00, \"Ahmedabad\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9777886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.createDataFrame(data4, schema = df4_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ee704d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+---------+\n",
      "| id|  name| salary|     city|\n",
      "+---+------+-------+---------+\n",
      "|  6|Aniket|70000.0|     Pune|\n",
      "|  7| Swati|85000.5|Hyderabad|\n",
      "|  3| Anmol|95000.0|Ahmedabad|\n",
      "+---+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "002d2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.withColumn(\"age\", lit(\"Null\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2c3634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+---------+----+\n",
      "| id|  name| salary|     city| age|\n",
      "+---+------+-------+---------+----+\n",
      "|  6|Aniket|70000.0|     Pune|Null|\n",
      "|  7| Swati|85000.5|Hyderabad|Null|\n",
      "|  3| Anmol|95000.0|Ahmedabad|Null|\n",
      "+---+------+-------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ad2a07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_order = [\"id\", \"name\", \"age\", \"salary\", \"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f09d360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.select(new_column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e72344c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+-------+---------+\n",
      "| id|  name| age| salary|     city|\n",
      "+---+------+----+-------+---------+\n",
      "|  6|Aniket|Null|70000.0|     Pune|\n",
      "|  7| Swati|Null|85000.5|Hyderabad|\n",
      "|  3| Anmol|Null|95000.0|Ahmedabad|\n",
      "+---+------+----+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "02fbbb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df1.union(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f6cda66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+--------+-------------+\n",
      "| id|  name| age|  salary|         city|\n",
      "+---+------+----+--------+-------------+\n",
      "|  1| Alice|  28| 70000.0|     New York|\n",
      "|  2|   Bob|  34| 85000.5|  Los Angeles|\n",
      "|  3| Cathy|  29| 95000.0|      Chicago|\n",
      "|  4| David|  41|123000.0|San Francisco|\n",
      "|  5|   Eva|  36|110000.0|      Seattle|\n",
      "|  6|Aniket|Null| 70000.0|         Pune|\n",
      "|  7| Swati|Null| 85000.5|    Hyderabad|\n",
      "|  3| Anmol|Null| 95000.0|    Ahmedabad|\n",
      "+---+------+----+--------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 55296)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d523f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
