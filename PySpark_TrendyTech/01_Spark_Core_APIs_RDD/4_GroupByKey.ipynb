{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2558de21",
   "metadata": {},
   "source": [
    "<H3> reduceByKey() vs groupByKey() </H3>\n",
    "\n",
    "<p> <b> reduceByKey() </b>: The data is locally aggregated first, then this locally aggregated data is shuffled and colocated based on the key. After shuffling, the data is further aggregated to produce the final result. This method efficiently uses all the nodes and reduces the amount of data shuffled.</p>\n",
    "\n",
    "<p><b> groupByKey() </b>: The data is first colocated based on the key, then it is aggregated. This approach has the following disadvantages:\n",
    "    <ol><li> Since the data is shuffled before aggregation, a large amount of data is shuffled, leading to high Network I/O.</li>\n",
    "        <li> After shuffling, the aggregation is done on one machine, resulting in inefficient use of the cluster's compute capabilities. For example, if there are 9 keys (statuses, regions, etc.), they will be shuffled to 9 or fewer machines based on the hashing algorithm and processed on these 9 machines. If the cluster has 100 nodes, the compute capabilities of 91 machines are not utilized.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7af0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port', '0'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b409fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_rdd = spark.sparkContext.textFile(\"/public/trendytech/retail_db/orders/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba14b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_rdd = orders_rdd.map(lambda line: (line.split(\",\")[3], line.split(\",\")[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afce8cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CLOSED', '11599'),\n",
       " ('PENDING_PAYMENT', '256'),\n",
       " ('COMPLETE', '12111'),\n",
       " ('CLOSED', '8827'),\n",
       " ('COMPLETE', '11318'),\n",
       " ('COMPLETE', '7130'),\n",
       " ('COMPLETE', '4530'),\n",
       " ('PROCESSING', '2911'),\n",
       " ('PENDING_PAYMENT', '5657'),\n",
       " ('PENDING_PAYMENT', '5648')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c340dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_rdd = mapped_rdd.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cb99e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CLOSED', <pyspark.resultiterable.ResultIterable at 0x7f7a855f8f60>),\n",
       " ('CANCELED', <pyspark.resultiterable.ResultIterable at 0x7f7a855f81d0>),\n",
       " ('PENDING_PAYMENT',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7f7a85149160>),\n",
       " ('COMPLETE', <pyspark.resultiterable.ResultIterable at 0x7f7a850e9e80>),\n",
       " ('PROCESSING', <pyspark.resultiterable.ResultIterable at 0x7f7a851491d0>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e90f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grouped_rdd.map(lambda x: (x[0],len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "794d8969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CLOSED', 7556),\n",
       " ('CANCELED', 1428),\n",
       " ('PENDING_PAYMENT', 15030),\n",
       " ('COMPLETE', 22899),\n",
       " ('PROCESSING', 8275),\n",
       " ('PAYMENT_REVIEW', 729),\n",
       " ('PENDING', 7610),\n",
       " ('ON_HOLD', 3798),\n",
       " ('SUSPECTED_FRAUD', 1558)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb079bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
