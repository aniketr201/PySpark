{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e19244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ca6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "appName(\"SparkTransformations\"). \\\n",
    "config(\"spark.sql.warehouse.dir\",\"/user/itv012857/warehourse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master(\"yarn\"). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7330b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g02.itversity.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkTransformations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8d4fdbdd68>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953f8bc",
   "metadata": {},
   "source": [
    "<h3> 1. Create a dataframe with the following data </h3>\n",
    "\n",
    "[(\"Spring\",12.3),(\"Summer\",10.5),(\"Autumn\",8.2),(\"Winter\",15.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba959533",
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed_list = [(\"Spring\",12.3),\n",
    "        (\"Summer\",10.5),\n",
    "        (\"Autumn\",8.2),\n",
    "        (\"Winter\",15.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6894a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spring', 12.3), ('Summer', 10.5), ('Autumn', 8.2), ('Winter', 15.1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windspeed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f5d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed_df = spark.createDataFrame(windspeed_list).toDF(\"season\",\"windspeed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5798be9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|season|windspeed|\n",
      "+------+---------+\n",
      "|Spring|     12.3|\n",
      "|Summer|     10.5|\n",
      "|Autumn|      8.2|\n",
      "|Winter|     15.1|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windspeed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b52c17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: string (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windspeed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064fee1",
   "metadata": {},
   "source": [
    "<H4> Alternate way is to define the schema and then pass the schema while creating the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6373683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed_schema = \"season string, windspeed float\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ea7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed_df2 = spark.createDataFrame(windspeed_list,schema = windspeed_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc5ec87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|season|windspeed|\n",
      "+------+---------+\n",
      "|Spring|     12.3|\n",
      "|Summer|     10.5|\n",
      "|Autumn|      8.2|\n",
      "|Winter|     15.1|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windspeed_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876bd540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: string (nullable = true)\n",
      " |-- windspeed: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windspeed_df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b18761c",
   "metadata": {},
   "source": [
    "<h4> 2. Consider the library management dataset located at the following path: /public/trendytech/datasets/library_data.json. Using PySpark, load the data into a DataFrame and enforce schema using StructType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acdb5356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"library_name\": \"Central Library\",\"location\": \"City Center\",\"books\": [{\"book_id\": \"B001\",\"book_name\": \"The Great Gatsby\",\"author\": \"F. Scott Fitzgerald\",\"copies_available\": 5},{\"book_id\": \"B002\",\"book_name\": \"To Kill a Mockingbird\",\"author\": \"Harper Lee\",\"copies_available\": 3}],\"members\": [{\"member_id\": \"M001\",\"member_name\": \"John Smith\",\"age\": 28,\"books_borrowed\": [\"B001\"]},{\"member_id\": \"M002\",\"member_name\": \"Emma Johnson\",\"age\": 35,\"books_borrowed\": []}]},\n",
      "{\"library_name\": \"Community Library\",\"location\": \"Suburb\",\"books\": [{\"book_id\": \"B003\",\"book_name\": \"1984\",\"author\": \"George Orwell\",\"copies_available\": 2},{\"book_id\": \"B004\",\"book_name\": \"Pride and Prejudice\",\"author\": \"Jane Austen\",\"copies_available\": 4}],\"members\": [{\"member_id\": \"M003\",\"member_name\": \"Michael Brown\",\"age\": 42,\"books_borrowed\": [\"B003\",\"B004\"]},{\"member_id\": \"M004\",\"member_name\": \"Sophia Davis\",\"age\": 31,\"books_borrowed\": [\"B004\"]}]}\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -head /public/trendytech/datasets/library_data.json"
   ]
  },
  {
   "cell_type": "raw",
   "id": "373bc9ea",
   "metadata": {},
   "source": [
    "JSON Structure is:-\n",
    "    - library_name: string\n",
    "    - location: string\n",
    "    - books: [Array] - Multiple Books in the Library\n",
    "        -- book_id: string\n",
    "        -- book_name: string\n",
    "        -- author: string\n",
    "        -- copies_available: int\n",
    "    - members: [Array] - Multiple Members associated with the Library\n",
    "        -- member_id: string\n",
    "        -- member_name: string\n",
    "        -- age: int\n",
    "        -- books_borrowed: Array of borrowed books IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05ab97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "923c4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "library_schema = StructType(\n",
    "                            [  StructField(\"library_name\",StringType(),nullable=False),\n",
    "                               StructField(\"location\",StringType(),nullable=False),\n",
    "                               StructField(\"books\",ArrayType(StructType(\n",
    "                                                                [StructField(\"book_id\",StringType(),nullable=False),\n",
    "                                                                StructField(\"book_name\",StringType(),nullable=False),\n",
    "                                                                StructField(\"author\",StringType(),nullable=False),\n",
    "                                                                StructField(\"copies_available\",IntegerType(),nullable=False)]\n",
    "                                                              ))\n",
    "                                               , nullable=False\n",
    "                                          ),\n",
    "                             StructField(\"members\",ArrayType(StructType(\n",
    "                                                                [StructField(\"member_id\",StringType(),nullable=False),\n",
    "                                                                StructField(\"member_name\",StringType(),nullable=False),\n",
    "                                                                StructField(\"age\",IntegerType(),nullable=False),\n",
    "                                                                StructField(\"books_borrowed\",ArrayType(StringType()),nullable=False)]\n",
    "                                                              ))\n",
    "                                               , nullable=False)\n",
    "                            ]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18e20093",
   "metadata": {},
   "outputs": [],
   "source": [
    "library_df = spark.read.json(\"/public/trendytech/datasets/library_data.json\", schema=library_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17ea622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+\n",
      "|library_name     |location   |books                                                                                           |members                                                                    |\n",
      "+-----------------+-----------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+\n",
      "|Central Library  |City Center|[{B001, The Great Gatsby, F. Scott Fitzgerald, 5}, {B002, To Kill a Mockingbird, Harper Lee, 3}]|[{M001, John Smith, 28, [B001]}, {M002, Emma Johnson, 35, []}]             |\n",
      "|Community Library|Suburb     |[{B003, 1984, George Orwell, 2}, {B004, Pride and Prejudice, Jane Austen, 4}]                   |[{M003, Michael Brown, 42, [B003, B004]}, {M004, Sophia Davis, 31, [B004]}]|\n",
      "+-----------------+-----------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74c4a795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- library_name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- books: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- book_id: string (nullable = true)\n",
      " |    |    |-- book_name: string (nullable = true)\n",
      " |    |    |-- author: string (nullable = true)\n",
      " |    |    |-- copies_available: integer (nullable = true)\n",
      " |-- members: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- member_id: string (nullable = true)\n",
      " |    |    |-- member_name: string (nullable = true)\n",
      " |    |    |-- age: integer (nullable = true)\n",
      " |    |    |-- books_borrowed: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b42d9",
   "metadata": {},
   "source": [
    "<H3> 3. Given the dataset (/public/trendytech/datasets/train.csv), create a DataFrame using PySpark and perform the following operations: </H3>\n",
    "\n",
    "a) Drop the columns passenger_name and age from the dataset.\n",
    "\n",
    "b) Count the number of rows after removing duplicates of columns train_number and ticket_number.\n",
    "\n",
    "c) Count the number of unique train names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82eb93d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_number,train_name,seats_available,passenger_name,age,ticket_number,seat_number\n",
      "123,Express,100,John,25,T123,A1\n",
      "123,Express,100,Emma,30,T124,B2\n",
      "456,Superfast,150,Michael,35,T125,C3\n",
      "456,Superfast,150,Sophia,40,T126,D4\n",
      "789,Local,50,William,28,T127,E5\n",
      "789,Local,50,Sophia,32,T128,F6\n",
      "789,Local,50,Oliver,45,T129,G7\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -head /public/trendytech/datasets/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acd8c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_schema = \"train_number int, train_name string, seats_available int,passenger_name string, age int,ticket_number string,seat_number string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e60551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.csv(\"/public/trendytech/datasets/train.csv\",header = True, schema= train_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f80e171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|train_number|train_name|seats_available|passenger_name|age|ticket_number|seat_number|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|         123|   Express|            100|          John| 25|         T123|         A1|\n",
      "|         123|   Express|            100|          Emma| 30|         T124|         B2|\n",
      "|         456| Superfast|            150|       Michael| 35|         T125|         C3|\n",
      "|         456| Superfast|            150|        Sophia| 40|         T126|         D4|\n",
      "|         789|     Local|             50|       William| 28|         T127|         E5|\n",
      "|         789|     Local|             50|        Sophia| 32|         T128|         F6|\n",
      "|         789|     Local|             50|        Oliver| 45|         T129|         G7|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ed11031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- train_number: integer (nullable = true)\n",
      " |-- train_name: string (nullable = true)\n",
      " |-- seats_available: integer (nullable = true)\n",
      " |-- passenger_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- ticket_number: string (nullable = true)\n",
      " |-- seat_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6aa11a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df.drop(\"passenger_name\",\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6a9987c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+-------------+-----------+\n",
      "|train_number|train_name|seats_available|ticket_number|seat_number|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "|         123|   Express|            100|         T123|         A1|\n",
      "|         123|   Express|            100|         T124|         B2|\n",
      "|         456| Superfast|            150|         T125|         C3|\n",
      "|         456| Superfast|            150|         T126|         D4|\n",
      "|         789|     Local|             50|         T127|         E5|\n",
      "|         789|     Local|             50|         T128|         F6|\n",
      "|         789|     Local|             50|         T129|         G7|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e66c96a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ea4c2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dropDuplicates([\"train_number\",\"ticket_number\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "790415fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.select(\"train_name\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e50a9",
   "metadata": {},
   "source": [
    "<H4> 4. You are working as a Data Engineer in a large retail company. </H4>The company has a dataset named \"sales_data.json\" that contains sales records from various stores. The dataset is stored in JSON format and may have some corrupt or malformed records due to occasional data quality issues. Your task is to read the \"sales_data.json\" dataset (/public/trendytech/datasets/sales_data.json) using PySpark, utilizing different read modes to handle corrupt records. You need to create a DataFrame using PySpark and perform the following operations:\n",
    "\n",
    "    Read the dataset using the \"permissive\" mode and count the number of records read.\n",
    "\n",
    "    Read the dataset using the \"dropmalformed\" mode and display the number of malformed records.\n",
    "\n",
    "    Read the dataset using the \"failfast\" mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4adc3ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"store_id\": 1, \"product\": \"Apple\", \"quantity\": 10, \"revenue\": 100.0}\n",
      "{\"store_id\": 2, \"product\": \"Banana\", \"quantity\": 15, \"revenue\": 75.0}\n",
      "{\"store_id\": 3, \"product\": \"Orange\", \"quantity\": 12, \"revenue\": 90.0}\n",
      "{\"store_id\": 4, \"product\": \"Mango\", \"quantity\": 8, \"revenue\": 120.0}\n",
      "{\"store_id\": 5, \"product\": \"Grape\", \"quantity\": 20, \"revenue\": 150.0}\n",
      "{\"store_id\": 6, \"product\": \"Watermelon\", \"quantity\": 5, \"revenue\": 50.0}\n",
      "{\"store_id\": 7, \"product\": \"Strawberry\", \"quantity\": 18, \"revenue\": 108.0}\n",
      "{\"store_id\": 8, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0}\n",
      "{\"store_id\": 9, \"product\": \"Cherry\", \"quantity\": 7, \"revenue\": 105.0}\n",
      "{\"store_id\": 10, \"product\": \"Pear\", \"quantity\": 9, \"revenue\": 81.0}\n",
      "{\"store_id\": 11, \"product\": \"Blueberry\", \"quantity\": 11, \"revenue\": 88.0}\n",
      "{\"store_id\": 12, \"product\": \"Kiwi\", \"quantity\": 16, \"revenue\": 128.0}\n",
      "{\"store_id\": 13, \"product\": \"Peach\", \"quantity\": 13, \"revenue\": 91.0}\n",
      "{\"store_id\": 14, \"product\": \"Plum\", \"quantity\": 6, \"revenue\": 54.0}\n",
      "{\"store_id\": 15, \"product\": \"Lemon\", \"quantity\": 10, \"revenue\": 70.0}\n",
      "{\"store_id\": 16, \"product\": \"Raspberry\", \"quantity\": 17, \"revenue\": 136.0}\n",
      "{\"store_id\": 17, \"product\": \"Coconut\", \"quantity\": 4, \"revenue\": 80.0}\n",
      "{\"store_id\": 18, \"product\": \"Avocado\", \"quantity\": 11, \"revenue\": 99.0}\n",
      "{\"store_id\": 19, \"product\": \"Blackberry\", \"quantity\": 8, \"revenue\": 64.0}\n",
      "{\"store_id\": 20, \"product\": \"G\", \"quantity\": \"Invalid\", \"revenue\": \"NaN\"}\n",
      "{\"store_id\": 21, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0\n",
      "{\"store_id\": 22, \"product\": \"Watermelon\", \"quantity\": 5, \"revenue\": \"Invalid\"}\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /public/trendytech/datasets/sales_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e25da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_schema = \"store_id int, product string, quantity int, revenue float\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "032bffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_permissive = spark.read.json(\"/public/trendytech/datasets/sales_data.json\", \\\n",
    "                                   schema= sales_schema, \\\n",
    "                                   mode = \"permissive\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcbbb6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_permissive.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39ef190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-------+\n",
      "|store_id|   product|quantity|revenue|\n",
      "+--------+----------+--------+-------+\n",
      "|       1|     Apple|      10|  100.0|\n",
      "|       2|    Banana|      15|   75.0|\n",
      "|       3|    Orange|      12|   90.0|\n",
      "|       4|     Mango|       8|  120.0|\n",
      "|       5|     Grape|      20|  150.0|\n",
      "|       6|Watermelon|       5|   50.0|\n",
      "|       7|Strawberry|      18|  108.0|\n",
      "|       8| Pineapple|      14|  140.0|\n",
      "|       9|    Cherry|       7|  105.0|\n",
      "|      10|      Pear|       9|   81.0|\n",
      "|      11| Blueberry|      11|   88.0|\n",
      "|      12|      Kiwi|      16|  128.0|\n",
      "|      13|     Peach|      13|   91.0|\n",
      "|      14|      Plum|       6|   54.0|\n",
      "|      15|     Lemon|      10|   70.0|\n",
      "|      16| Raspberry|      17|  136.0|\n",
      "|      17|   Coconut|       4|   80.0|\n",
      "|      18|   Avocado|      11|   99.0|\n",
      "|      19|Blackberry|       8|   64.0|\n",
      "|      20|         G|    null|    NaN|\n",
      "|    null|      null|    null|   null|\n",
      "|      22|Watermelon|       5|   null|\n",
      "+--------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_permissive.show(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80f1e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_dropmalformed =  spark.read.json(\"/public/trendytech/datasets/sales_data.json\", \\\n",
    "                                   schema= sales_schema, \\\n",
    "                                   mode = \"dropmalformed\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73c78974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_dropmalformed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b44e8a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-------+\n",
      "|store_id|   product|quantity|revenue|\n",
      "+--------+----------+--------+-------+\n",
      "|       1|     Apple|      10|  100.0|\n",
      "|       2|    Banana|      15|   75.0|\n",
      "|       3|    Orange|      12|   90.0|\n",
      "|       4|     Mango|       8|  120.0|\n",
      "|       5|     Grape|      20|  150.0|\n",
      "|       6|Watermelon|       5|   50.0|\n",
      "|       7|Strawberry|      18|  108.0|\n",
      "|       8| Pineapple|      14|  140.0|\n",
      "|       9|    Cherry|       7|  105.0|\n",
      "|      10|      Pear|       9|   81.0|\n",
      "|      11| Blueberry|      11|   88.0|\n",
      "|      12|      Kiwi|      16|  128.0|\n",
      "|      13|     Peach|      13|   91.0|\n",
      "|      14|      Plum|       6|   54.0|\n",
      "|      15|     Lemon|      10|   70.0|\n",
      "|      16| Raspberry|      17|  136.0|\n",
      "|      17|   Coconut|       4|   80.0|\n",
      "|      18|   Avocado|      11|   99.0|\n",
      "|      19|Blackberry|       8|   64.0|\n",
      "+--------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_dropmalformed.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c74dcfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_failfast =  spark.read.json(\"/public/trendytech/datasets/sales_data.json\", \\\n",
    "                                   schema= sales_schema, \\\n",
    "                                   mode = \"failfast\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8853015",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o173.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 4 times, most recent failure: Lost task 0.3 in stage 21.0 (TID 422) (w01.itversity.com executor 2): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:70)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$9(JsonDataSource.scala:144)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse a value for data type int (current token: VALUE_STRING).\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:492)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$7(JsonDataSource.scala:140)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 23 more\nCaused by: java.lang.RuntimeException: Failed to parse a value for data type int (current token: VALUE_STRING).\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:375)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:355)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$4$1.applyOrElse(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$4$1.applyOrElse(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:343)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$4(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:397)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:96)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:95)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:343)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:95)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:467)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2622)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:462)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-0c7b4007df5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msales_failfast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o173.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 4 times, most recent failure: Lost task 0.3 in stage 21.0 (TID 422) (w01.itversity.com executor 2): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:70)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$9(JsonDataSource.scala:144)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse a value for data type int (current token: VALUE_STRING).\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:492)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$7(JsonDataSource.scala:140)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 23 more\nCaused by: java.lang.RuntimeException: Failed to parse a value for data type int (current token: VALUE_STRING).\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:375)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:355)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$4$1.applyOrElse(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$4$1.applyOrElse(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:343)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$4(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:397)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:96)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:95)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:343)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:95)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:467)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2622)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:462)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "sales_failfast.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1daeec0",
   "metadata": {},
   "source": [
    "<H3> 5. Questions regarding hospital dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84cce159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_id,admission_date,discharge_date,diagnosis,doctor_id,total_cost\n",
      "1,01-01-2022,2022-01-10,Pneumonia,101,5000.00\n",
      "2,02-05-2022,2022-02-09,Appendicitis,102,7000.00\n",
      "3,03-12-2022,2022-03-18,Fractured Arm,103,3500.00\n",
      "4,04-02-2022,2022-04-08,Heart Attack,104,15000.00\n",
      "5,05-05-2022,2022-05-07,Influenza,105,2500.00\n",
      "6,06-10-2022,2022-06-15,Appendicitis,106,8000.00\n",
      "7,07-20-2022,2022-07-25,Pneumonia,107,5500.00\n",
      "8,08-25-2022,2022-09-01,Heart Attack,108,20000.00\n",
      "9,09-15-2022,2022-09-22,Fractured Leg,109,6000.00\n",
      "10,10-05-2022,2022-10-10,Appendicitis,110,7500.00\n",
      "11,11-02-2022,2022-11-05,Influenza,111,2800.00\n",
      "12,12-10-2022,2022-12-18,Pneumonia,112,6000.00\n",
      "13,01-02-2023,2023-01-09,Heart Attack,113,18000.00\n",
      "14,02-14-2023,2023-02-18,Appendicitis,114,7200.00\n",
      "15,03-20-2023,2023-03-28,Fractured Arm,115,3800.00\n",
      "16,04-05-2023,2023-04-11,Influenza,116,2700.00\n",
      "17,05-08-2023,2023-05-11,Heart Attack,117,16000.00\n",
      "18,06-15-2023,2023-06-20,Pneumonia,118,4800.00\n",
      "19,07-22-2023,2023-07-27,Fractured Leg,119,6500.00\n",
      "20,0"
     ]
    }
   ],
   "source": [
    "! hadoop fs -head /public/trendytech/datasets/hospital.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d899ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df = spark.read. \\\n",
    "format(\"csv\"). \\\n",
    "option(\"header\",\"true\"). \\\n",
    "option(\"inferSchema\",\"true\"). \\\n",
    "load(\"/public/trendytech/datasets/hospital.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01c7b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|         1|    01-01-2022|    2022-01-10|    Pneumonia|      101|    5000.0|\n",
      "|         2|    02-05-2022|    2022-02-09| Appendicitis|      102|    7000.0|\n",
      "|         3|    03-12-2022|    2022-03-18|Fractured Arm|      103|    3500.0|\n",
      "|         4|    04-02-2022|    2022-04-08| Heart Attack|      104|   15000.0|\n",
      "|         5|    05-05-2022|    2022-05-07|    Influenza|      105|    2500.0|\n",
      "|         6|    06-10-2022|    2022-06-15| Appendicitis|      106|    8000.0|\n",
      "|         7|    07-20-2022|    2022-07-25|    Pneumonia|      107|    5500.0|\n",
      "|         8|    08-25-2022|    2022-09-01| Heart Attack|      108|   20000.0|\n",
      "|         9|    09-15-2022|    2022-09-22|Fractured Leg|      109|    6000.0|\n",
      "|        10|    10-05-2022|    2022-10-10| Appendicitis|      110|    7500.0|\n",
      "|        11|    11-02-2022|    2022-11-05|    Influenza|      111|    2800.0|\n",
      "|        12|    12-10-2022|    2022-12-18|    Pneumonia|      112|    6000.0|\n",
      "|        13|    01-02-2023|    2023-01-09| Heart Attack|      113|   18000.0|\n",
      "|        14|    02-14-2023|    2023-02-18| Appendicitis|      114|    7200.0|\n",
      "|        15|    03-20-2023|    2023-03-28|Fractured Arm|      115|    3800.0|\n",
      "|        16|    04-05-2023|    2023-04-11|    Influenza|      116|    2700.0|\n",
      "|        17|    05-08-2023|    2023-05-11| Heart Attack|      117|   16000.0|\n",
      "|        18|    06-15-2023|    2023-06-20|    Pneumonia|      118|    4800.0|\n",
      "|        19|    07-22-2023|    2023-07-27|Fractured Leg|      119|    6500.0|\n",
      "|        20|    08-10-2023|    2023-08-16| Appendicitis|      120|    7800.0|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6cc2cc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: integer (nullable = true)\n",
      " |-- admission_date: string (nullable = true)\n",
      " |-- discharge_date: string (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      " |-- doctor_id: integer (nullable = true)\n",
      " |-- total_cost: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3793d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df2 = hospital_df.drop(\"doctor_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed05d4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|total_cost|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "|         1|    01-01-2022|    2022-01-10|    Pneumonia|    5000.0|\n",
      "|         2|    02-05-2022|    2022-02-09| Appendicitis|    7000.0|\n",
      "|         3|    03-12-2022|    2022-03-18|Fractured Arm|    3500.0|\n",
      "|         4|    04-02-2022|    2022-04-08| Heart Attack|   15000.0|\n",
      "|         5|    05-05-2022|    2022-05-07|    Influenza|    2500.0|\n",
      "|         6|    06-10-2022|    2022-06-15| Appendicitis|    8000.0|\n",
      "|         7|    07-20-2022|    2022-07-25|    Pneumonia|    5500.0|\n",
      "|         8|    08-25-2022|    2022-09-01| Heart Attack|   20000.0|\n",
      "|         9|    09-15-2022|    2022-09-22|Fractured Leg|    6000.0|\n",
      "|        10|    10-05-2022|    2022-10-10| Appendicitis|    7500.0|\n",
      "|        11|    11-02-2022|    2022-11-05|    Influenza|    2800.0|\n",
      "|        12|    12-10-2022|    2022-12-18|    Pneumonia|    6000.0|\n",
      "|        13|    01-02-2023|    2023-01-09| Heart Attack|   18000.0|\n",
      "|        14|    02-14-2023|    2023-02-18| Appendicitis|    7200.0|\n",
      "|        15|    03-20-2023|    2023-03-28|Fractured Arm|    3800.0|\n",
      "|        16|    04-05-2023|    2023-04-11|    Influenza|    2700.0|\n",
      "|        17|    05-08-2023|    2023-05-11| Heart Attack|   16000.0|\n",
      "|        18|    06-15-2023|    2023-06-20|    Pneumonia|    4800.0|\n",
      "|        19|    07-22-2023|    2023-07-27|Fractured Leg|    6500.0|\n",
      "|        20|    08-10-2023|    2023-08-16| Appendicitis|    7800.0|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0a46e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df3 = hospital_df2.withColumnRenamed(\"total_cost\",\"hospital_bill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "887ccee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|         1|    01-01-2022|    2022-01-10|    Pneumonia|       5000.0|\n",
      "|         2|    02-05-2022|    2022-02-09| Appendicitis|       7000.0|\n",
      "|         3|    03-12-2022|    2022-03-18|Fractured Arm|       3500.0|\n",
      "|         4|    04-02-2022|    2022-04-08| Heart Attack|      15000.0|\n",
      "|         5|    05-05-2022|    2022-05-07|    Influenza|       2500.0|\n",
      "|         6|    06-10-2022|    2022-06-15| Appendicitis|       8000.0|\n",
      "|         7|    07-20-2022|    2022-07-25|    Pneumonia|       5500.0|\n",
      "|         8|    08-25-2022|    2022-09-01| Heart Attack|      20000.0|\n",
      "|         9|    09-15-2022|    2022-09-22|Fractured Leg|       6000.0|\n",
      "|        10|    10-05-2022|    2022-10-10| Appendicitis|       7500.0|\n",
      "|        11|    11-02-2022|    2022-11-05|    Influenza|       2800.0|\n",
      "|        12|    12-10-2022|    2022-12-18|    Pneumonia|       6000.0|\n",
      "|        13|    01-02-2023|    2023-01-09| Heart Attack|      18000.0|\n",
      "|        14|    02-14-2023|    2023-02-18| Appendicitis|       7200.0|\n",
      "|        15|    03-20-2023|    2023-03-28|Fractured Arm|       3800.0|\n",
      "|        16|    04-05-2023|    2023-04-11|    Influenza|       2700.0|\n",
      "|        17|    05-08-2023|    2023-05-11| Heart Attack|      16000.0|\n",
      "|        18|    06-15-2023|    2023-06-20|    Pneumonia|       4800.0|\n",
      "|        19|    07-22-2023|    2023-07-27|Fractured Leg|       6500.0|\n",
      "|        20|    08-10-2023|    2023-08-16| Appendicitis|       7800.0|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f78951",
   "metadata": {},
   "source": [
    "#### Calculate the duration of stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f479b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, datediff, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cef7100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df4 = hospital_df3. \\\n",
    "                withColumn(\"admission_date\",to_date(\"admission_date\",\"MM-dd-yyyy\")). \\\n",
    "                withColumn(\"discharge_date\",to_date(\"discharge_date\",\"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "846826ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|       7000.0|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|       3500.0|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|      15000.0|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|       2500.0|\n",
      "|         6|    2022-06-10|    2022-06-15| Appendicitis|       8000.0|\n",
      "|         7|    2022-07-20|    2022-07-25|    Pneumonia|       5500.0|\n",
      "|         8|    2022-08-25|    2022-09-01| Heart Attack|      20000.0|\n",
      "|         9|    2022-09-15|    2022-09-22|Fractured Leg|       6000.0|\n",
      "|        10|    2022-10-05|    2022-10-10| Appendicitis|       7500.0|\n",
      "|        11|    2022-11-02|    2022-11-05|    Influenza|       2800.0|\n",
      "|        12|    2022-12-10|    2022-12-18|    Pneumonia|       6000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      18000.0|\n",
      "|        14|    2023-02-14|    2023-02-18| Appendicitis|       7200.0|\n",
      "|        15|    2023-03-20|    2023-03-28|Fractured Arm|       3800.0|\n",
      "|        16|    2023-04-05|    2023-04-11|    Influenza|       2700.0|\n",
      "|        17|    2023-05-08|    2023-05-11| Heart Attack|      16000.0|\n",
      "|        18|    2023-06-15|    2023-06-20|    Pneumonia|       4800.0|\n",
      "|        19|    2023-07-22|    2023-07-27|Fractured Leg|       6500.0|\n",
      "|        20|    2023-08-10|    2023-08-16| Appendicitis|       7800.0|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d9fdafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: integer (nullable = true)\n",
      " |-- admission_date: date (nullable = true)\n",
      " |-- discharge_date: date (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      " |-- doctor_id: integer (nullable = true)\n",
      " |-- total_cost: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f6c4b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df5 = hospital_df4. \\\n",
    "    withColumn(\"duration_of_stay\",datediff(\"discharge_date\",\"admission_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7dd39e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|               9|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|       7000.0|               4|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|       3500.0|               6|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|      15000.0|               6|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|       2500.0|               2|\n",
      "|         6|    2022-06-10|    2022-06-15| Appendicitis|       8000.0|               5|\n",
      "|         7|    2022-07-20|    2022-07-25|    Pneumonia|       5500.0|               5|\n",
      "|         8|    2022-08-25|    2022-09-01| Heart Attack|      20000.0|               7|\n",
      "|         9|    2022-09-15|    2022-09-22|Fractured Leg|       6000.0|               7|\n",
      "|        10|    2022-10-05|    2022-10-10| Appendicitis|       7500.0|               5|\n",
      "|        11|    2022-11-02|    2022-11-05|    Influenza|       2800.0|               3|\n",
      "|        12|    2022-12-10|    2022-12-18|    Pneumonia|       6000.0|               8|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      18000.0|               7|\n",
      "|        14|    2023-02-14|    2023-02-18| Appendicitis|       7200.0|               4|\n",
      "|        15|    2023-03-20|    2023-03-28|Fractured Arm|       3800.0|               8|\n",
      "|        16|    2023-04-05|    2023-04-11|    Influenza|       2700.0|               6|\n",
      "|        17|    2023-05-08|    2023-05-11| Heart Attack|      16000.0|               3|\n",
      "|        18|    2023-06-15|    2023-06-20|    Pneumonia|       4800.0|               5|\n",
      "|        19|    2023-07-22|    2023-07-27|Fractured Leg|       6500.0|               5|\n",
      "|        20|    2023-08-10|    2023-08-16| Appendicitis|       7800.0|               6|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8624931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df6 = hospital_df5.withColumn(\"adjusted_total_cost\", \n",
    "                                        expr(\"CASE \\\n",
    "                                                 WHEN diagnosis='Heart Attack' THEN hospital_bill * 1.5 \\\n",
    "                                                 WHEN diagnosis='Appendicitis' THEN hospital_bill * 1.2 \\\n",
    "                                                 ELSE hospital_bill \\\n",
    "                                                 END \\\n",
    "                                             \")\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c11c5b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|adjusted_total_cost|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|               9|             5000.0|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|       7000.0|               4|             8400.0|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|       3500.0|               6|             3500.0|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|      15000.0|               6|            22500.0|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|       2500.0|               2|             2500.0|\n",
      "|         6|    2022-06-10|    2022-06-15| Appendicitis|       8000.0|               5|             9600.0|\n",
      "|         7|    2022-07-20|    2022-07-25|    Pneumonia|       5500.0|               5|             5500.0|\n",
      "|         8|    2022-08-25|    2022-09-01| Heart Attack|      20000.0|               7|            30000.0|\n",
      "|         9|    2022-09-15|    2022-09-22|Fractured Leg|       6000.0|               7|             6000.0|\n",
      "|        10|    2022-10-05|    2022-10-10| Appendicitis|       7500.0|               5|             9000.0|\n",
      "|        11|    2022-11-02|    2022-11-05|    Influenza|       2800.0|               3|             2800.0|\n",
      "|        12|    2022-12-10|    2022-12-18|    Pneumonia|       6000.0|               8|             6000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      18000.0|               7|            27000.0|\n",
      "|        14|    2023-02-14|    2023-02-18| Appendicitis|       7200.0|               4|             8640.0|\n",
      "|        15|    2023-03-20|    2023-03-28|Fractured Arm|       3800.0|               8|             3800.0|\n",
      "|        16|    2023-04-05|    2023-04-11|    Influenza|       2700.0|               6|             2700.0|\n",
      "|        17|    2023-05-08|    2023-05-11| Heart Attack|      16000.0|               3|            24000.0|\n",
      "|        18|    2023-06-15|    2023-06-20|    Pneumonia|       4800.0|               5|             4800.0|\n",
      "|        19|    2023-07-22|    2023-07-27|Fractured Leg|       6500.0|               5|             6500.0|\n",
      "|        20|    2023-08-10|    2023-08-16| Appendicitis|       7800.0|               6|             9360.0|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0a580369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+-------------------+\n",
      "|patient_id|    diagnosis|hospital_bill|adjusted_total_cost|\n",
      "+----------+-------------+-------------+-------------------+\n",
      "|         1|    Pneumonia|       5000.0|             5000.0|\n",
      "|         2| Appendicitis|       7000.0|             8400.0|\n",
      "|         3|Fractured Arm|       3500.0|             3500.0|\n",
      "|         4| Heart Attack|      15000.0|            22500.0|\n",
      "|         5|    Influenza|       2500.0|             2500.0|\n",
      "|         6| Appendicitis|       8000.0|             9600.0|\n",
      "|         7|    Pneumonia|       5500.0|             5500.0|\n",
      "|         8| Heart Attack|      20000.0|            30000.0|\n",
      "|         9|Fractured Leg|       6000.0|             6000.0|\n",
      "|        10| Appendicitis|       7500.0|             9000.0|\n",
      "|        11|    Influenza|       2800.0|             2800.0|\n",
      "|        12|    Pneumonia|       6000.0|             6000.0|\n",
      "|        13| Heart Attack|      18000.0|            27000.0|\n",
      "|        14| Appendicitis|       7200.0|             8640.0|\n",
      "|        15|Fractured Arm|       3800.0|             3800.0|\n",
      "|        16|    Influenza|       2700.0|             2700.0|\n",
      "|        17| Heart Attack|      16000.0|            24000.0|\n",
      "|        18|    Pneumonia|       4800.0|             4800.0|\n",
      "|        19|Fractured Leg|       6500.0|             6500.0|\n",
      "|        20| Appendicitis|       7800.0|             9360.0|\n",
      "+----------+-------------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df6.select(\"patient_id\", \"diagnosis\", \"hospital_bill\", \"adjusted_total_cost\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f305e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
